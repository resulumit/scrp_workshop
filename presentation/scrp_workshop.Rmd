---
title: "Automated Web Scraping with R"
subtitle: "Resul Umit"
institute: ""
date: "May 2021"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: false
---

class: inverse, center, middle

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, crayon.enabled = TRUE)
library(crayon)
library(fansi)
library(dplyr)
library(ggplot2)
library(stargazer)
library(countdown)
```

<style type="text/css">

.hljs-github .hljs {
    background: #e5e5e5;
}

.inline-c, remark-inline-code {
   background: #e5e5e5;
   border-radius: 3px;
   padding: 4px;
   font-family: 'Source Code Pro', 'Lucida Console', Monaco, monospace;
}


.yellow-h{
   background: #ffff88;
}


.out-t, remark-inline-code {
   background: #9fff9f;
   border-radius: 3px;
   padding: 4px;
   
}

.pull-left-c {
    float: left;
    width: 58%;
    
}

.pull-right-c {
    float: right;
    width: 38%;
    
}

.medium {
    font-size: 75%
    
}

.small {
    font-size: 50%
    }

.action {
    background-color: #f2eecb;
  
}


</style>


# Automated Web Scraping with R

<br>

### Resul Umit

### May 2021

.footnote[

[Skip intro &mdash; To the contents slide](#contents-slide).
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="mailto:resuluy@uio.no?subject=Twitter workshop">I can teach this workshop at your institution &mdash; Email me</a>.

]

---
## Who am I?

Resul Umit

- post-doctoral researcher in political science at the University of Oslo

- teaching and studying representation, elections, and parliaments
    - [a recent publication](https://doi.org/10.1177%2F1478929920967588):
    Parliamentary communication allowances do not increase electoral turnout or incumbentsâ€™ vote share

--

<br>

- teaching workshops, also on

  - [writing reproducible research papers](https://resulumit.com/blog/rmd-workshop/)
  - [version control and collaboration](https://resulumit.com/teaching/git_workshop.html)
  - [working with Twitter data](https://resulumit.com/teaching/twtr_workshop.html)
  - [creating academic websites](https://resulumit.com/teaching/rbd_workshop.html)
    
--

<br>

- more information available at [resulumit.com](https://resulumit.com/)

---
## The Workshop &mdash; Overview

- One day, on how to automate the process of extracting data from websites

  - 200+ slides, 75+ exercises
  - a [demonstration website](https://parliament-luzland.netlify.app/) for practice

--

<br>

- Designed for researchers with basic knowledge of R programming language

  - does not cover programming with R
      - e.g., we will use existing functions and packages   
<br>
  - ability to work with R will be very helpful
      - but not absolutely necessary &mdash; this ability can be developed during and after the workshop as well
        
---
## The Workshop &mdash; Motivation

- Data available on websites provide attractive opportunities for academic research

  - e.g., parliamentary websites were the main source of data for my PhD

--

<br>

- Acquiring such data requires 

  - either a lot of resources, such as time
  - or a set of skills, such as automated web scraping

--

<br>

- Typically, such skills are not part of academic training

  - for my PhD, I hand-visited close to 3000 webpages to collect data manually
      - on members of ten parliaments
      - multiple times, to update the dataset as needed

---
## The Workshop &mdash; Motivation &mdash; Aims

- To provide you with an understanding of what is ethically possible

  - we will cover a large breath of issues, not all of it is for long-term memory
      - hence the slides are designed for self study as well    
<br>
  - awareness of what is ethical and possible, `Google`, and perseverance are all you need

--

<br>

- To start you with acquiring and practicing the skills needed 

  - practice with the demonstration website
     - plenty of data, stable structure, and an ethical playground
  - start working on a real project

---
name: contents-slide

## The Workshop &mdash; Contents

.pull-left[

[Part 1. Getting the Tools Ready](#part1)
   - e.g., installing packages
   
[Part 2. Preliminary Considerations](#part2)
   - e.g., ethics of web scraping


[Part 3. Data Collection](#part3)
   - e.g., acquiring a user's tweets
   
[Part 4. Data Preperation](#part4)
   - e.g., creating a tidy dataset of tweets
   
]

.pull-right[
   

   
]

.footnote[

[To the list of references](#reference-slide).

] 

---
## The Workshop &mdash; Organisation

- ~~Sit in groups of two~~ Breakout in groups of two for exercises

  - participants learn as much from their partner as from instructors
  - switch partners after every other part
  - leave your breakout room manually, when everyone in the group is ready

<br> 

- Type, rather than copy and paste, the code that you will find on these slides

  - typing is a part of the learning process
  - slides are, and will remain, available at [resulumit.com/teaching/scrp_workshop.html](https://resulumit.com/teaching/scrp_workshop.html)

<br> 

- When you have a question

  - ask your partner
  - google together
  - ask me

---
class: action

## The Workshop &mdash; Organisation &mdash; Slides

Slides with this background colour indicate that your action is required, for

- setting the workshop up
    - e.g., installing R 
    
- completing the exercises
    - e.g., downloading tweets
    - there are 75+ exercises
    - these slides have countdown timers
        - as a guide, not to be followed strictly
    
`r countdown(minutes = 3, seconds = 00, top = 0)`

---
## The Workshop &mdash; Organisation &mdash; Slides

- Codes and texts that go in R console or scripts .inline-c[appear as such &mdash; in a different font, on gray background]    

    - long codes and texts will have their own line(s)

```{r, demonstration, eval=FALSE}
# read in  the tweets dataset
df <- read_rds("tweets.rds") %>%
  
# split the variable text, create a new variable called da_tweets   
  unnest_tokens(output = da_tweets, input = text, token = "tweets") %>%
 
# remove rows that match any of the stop words as stored in the stop_words dataset 
  anti_join(stop_words, by = c("da_tweets" = "word")) 
```

---
## The Workshop &mdash; Organisation &mdash; Slides

- Codes and texts that go in R console or scripts .inline-c[appear as such &mdash; in a different font, on gray background]    

    - long codes and texts will have their own line(s)

<br>

- Results that come out as output .out-t[appear as such &mdash; in the same font, on green background]    

    - except very obvious results, such as figures and tables
    
--

<br>

    
- Specific sections are .yellow-h[highlighted yellow as such] for emphasis

    - these could be for anything &mdash; codes and texts in input, results in output, and/or texts on slides
    
--

<br>

- The slides are designed for self-study as much as for the workshop

    - *accessible*, in substance and form, to go through on your own

---
name: part1
class: inverse, center, middle

# Part 1. Getting the Tools Ready

.footnote[

[Back to the contents slide](#contents-slide).

]

---
class: action

## Workshop Slides &mdash; Access on Your Browser

- Having the workshop slides<sup>*</sup> on your own machine might be helpful

  - flexibility to go back and forward on your own
      - especially while in a breakout room
  - ability to scroll across long codes on some slides

<br>

- Access at <https://resulumit.com/teaching/scrp_workshop.html>

  - will remain accessible after the workshop
  - might crash for some Safari users
      - if using a different browser application is not an option, view the [PDF version of the slides](https://github.com/resulumit/twtr_workshop/blob/master/presentation/twtr_workshop.pdf) on GitHub
  
.footnote[

<sup>*</sup> These slides are produced in R, with the `xaringan` package ([Xie, 2020](https://cran.r-project.org/web/packages/xaringan/xaringan.pdf)).

]

---
class: action

## Demonstration Website &mdash; Explore on Your Browser

- There is a demonstration website for this workshop

   - available at <https://parliament-luzland.netlify.app/>
   - includes fabricated data on the imaginary Parliament of Luzland
   - provides us with plenty of data, stable structure, and an ethical playground

<br>

- Using this demonstration website for practice is recommended

   - tailored to exercises, no ethical concern
   - but not compulsory &mdash; use a different one if you prefer so

<br>
 
- Explore the website now
   - see the four sections
   - click on the links to see an individual page for 
       - states, constituencies, members, and documents

`r countdown(minutes = 5, seconds = 00, top = 0)`
   
---
class: action

## R &mdash; Download from the Internet and Install

- Programming language of this workshop

  - created for data analysis, extending for other purposes
      - e.g., accessing websites
  - allows for all three steps in one environment
      - accessing websites; scraping and processing data
  - an alternative: [python](https://www.python.org/)

<br>

- Download R from [https://cloud.r-project.org](https://cloud.r-project.org)

  - optional, if you have it already installed &mdash; but then consider updating<sup>*</sup>
      - the `R.version.string` command checks the version of your copy
      - compare with the latest official release at [https://cran.r-project.org/sources.html](https://cran.r-project.org/sources.html)



.footnote[

<sup>*</sup> The same applies to all software that follows &mdash; consider updating if you have them already installed. This ensures everyone works with the latest, exactly the same, tools.

]


---
class: action

## RStudio &mdash; Download from the Internet and Install

- Optional, but highly recommended

  - facilitates working with R

<br>

- A popular integrated development environment (IDE) for R

  - an alternative: [GNU Emacs](https://www.gnu.org/software/emacs/)

<br>

- Download RStudio from [https://rstudio.com/products/rstudio/download](https://rstudio.com/products/rstudio/download)

  - choose the free version
  - to check for any updates, follow from the RStudio menu:

> `Help -> Check for Updates`

---
class: action
name: rstudio-project

## RStudio Project &mdash; Create from within RStudio 

- RStudio allows for dividing your work with R into separate projects

  - each project gets dedicated workspace, history, and source documents
  - [this page](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects) has more information on why projects are recommended

<br>

- Create a new RStudio project for for this workshop, following from the RStudio menu:

> `File -> New Project -> New Directory -> New Project`

<br>

- Choose a location for the project with `Browse...`

  - avoid choosing a synced location, e.g., `Dropbox`
      - likely to cause warning and/or error messages
      - if you must, pause syncing, or add an sync exclusion

---
class: action

## R Packages &mdash; Install from within RStudio<sup>*</sup>

Install the packages that we need

```{r eval=FALSE, tidy=FALSE}
install.packages(c("rvest", "RSelenium", "robotstxt", "polite",
                   "tidyverse", "tidytext"))
```


.footnote[

<sup>*</sup> You may already have a copy of one or more of these packages. In that case, I recommend updating by re-installing them now.

]

`r countdown(minutes = 2, seconds = 00, top = 0)`

---
class: action

## R Packages &mdash; Install from within RStudio

Install the packages that we need

```{r eval=FALSE, tidy=FALSE}
install.packages(c("rvest", "RSelenium", "robotstxt", "polite",
                   "tidyverse", "tidytext"))
```

<br>

We will use

- `rvest` ([Wickham, 2021](https://cran.r-project.org/web/packages/rvest/index.html)), for scraping websites

--
- `RSelenium` ([Harrison, 2020](https://cran.r-project.org/web/packages/RSelenium/index.html)), for browsing the web programmatically

--
- `robotstxt` ([Meissner & Ren, 2020](https://cran.r-project.org/web/packages/robotstxt/index.html)), for checking permissions to scrape websites

--
- `polite` ([Perepolkin, 2019](https://cran.r-project.org/web/packages/polite/index.html)), for compliance with permissions to scrape websites

---
class: action

## R Packages &mdash; Install from within RStudio

```{r eval=FALSE, tidy=FALSE}
install.packages(c("rvest", "RSelenium", "robotstxt", "polite",
                   "tidyverse", "tidytext"))

```

- `tidyverse` ([Wickham & RStudio 2019](https://cran.r-project.org/web/packages/tidyverse/index.html)), for various tasks

    - including data manipulation, visualisation
    - alternative: e.g., `base` R

--
<br>

- `tidytext` ([Robinson & Silge, 2021](https://cran.r-project.org/web/packages/tidytext/index.html)), for working with text as data


---
class: action

## Java &mdash; Download from the Internet and Install


- A language and software that `RSelenium` needs
   - for automation scripts

<br>

- Download Java from <https://www.java.com/en/download/>
   - requires restarting any browser that you might have open
   
---
class: action

## Chrome &mdash; Download from the Internet and Install


- A browser that facilitates web scraping
   - favoured by `RSelenium` and most programmers

<br>

- Download Chrome from <https://www.google.com/chrome/>

---
class: action

## ScrapeMate &mdash; Add Extension to Browser 

- An [open source software](https://github.com/hermit-crab/ScrapeMate) extension to Chrome, Firefox  

   - facilitates selecting what to scrape from a webpage
   - optional, but highly recommended

<br>

- Add the extension to your preferred browser
   
  - for Chrome, search at <https://chrome.google.com/webstore/category/extensions>
  - for Firefox, search at <https://addons.mozilla.org/>

<br>

- If you cannot use Chrome or Firefox
   - drag and drop the following link to your bookmarks bar: <a href="javascript:(function(){var%20s=document.createElement('div');s.innerHTML='Loading...';s.style.color='black';s.style.padding='20px';s.style.position='fixed';s.style.zIndex='9999';s.style.fontSize='3.0em';s.style.border='2px%20solid%20black';s.style.right='40px';s.style.top='40px';s.setAttribute('class','selector_gadget_loading');s.style.background='white';document.body.appendChild(s);s=document.createElement('script');s.setAttribute('type','text/javascript');s.setAttribute('src','https://dv0akt2986vzh.cloudfront.net/unstable/lib/selectorgadget.js');document.body.appendChild(s);})();">SelectorGadget</a>
       - another &mdash; similar but older &mdash; [open source software](https://github.com/cantino/selectorgadget) with the same functionality


---
## Other Resources<sup>*</sup>

- `RSelenium` vignettes

   - available at <https://cran.r-project.org/web/packages/RSelenium/vignettes/basics.html>

--

<br>

- R for Data Science ([Wickham & Grolemund, 2019](https://r4ds.had.co.nz/))
  
   - open access at <https://r4ds.had.co.nz>
   
--

<br>

- Text Mining with R: A Tidy Approach ([Silge & Robinson, 2017](https://www.tidytextmining.com/))
 
   - open access at [tidytextmining.com](https://www.tidytextmining.com/)
   - comes with [a course website](https://juliasilge.shinyapps.io/learntidytext/) where you can practice
   

   
.footnote[

<sup>*</sup> I recommend these to be consulted not during but after the workshop.

]

---
name: part2
class: inverse, center, middle

# Part 2. Preliminary Considerations

.footnote[

[Back to the contents slide](#contents-slide).

]

---
## Considerations &mdash; the Law

- Web scraping might be illegal     
<br>
   - depending on who is scraping what, why, how &mdash; and under which jurisdiction
   - reflect, and check, before you scrape

--

<br>

- Web scraping might be more likely to be illegal if, for example,      
<br>
   - it is harmful to the source
      - commercially
          - e.g., scraping a commercial website to create a rival website     
      - physically
          - e.g., scraping a website so hard and fast that it collapses    
<br>
   - it gathers data that is
      - under copyright
      - not meant for the public to see
      - then used for financial gain

---
## Considerations &mdash; the Ethics

- Web scraping might be unethical
   - even when it is legal
   - depending on who is scraping what, why, and how
   - reflect before you scrape

--

<br>

- Web scraping might be more likely to be unethical if, for example,    
<br>
   - it is &mdash; edging towards &mdash; illegal
   - it does not respect the restrictions
      - as defined in `robots.txt` files    
<br>
   - it harvests data 
       - that is otherwise available to download, e.g., through APIs
       - without purpose, at dangerous speed, repeatedly


---
## Considerations &mdash; the Ethics &mdash; `robots.txt`

- Most websites declare a robots exclusion protocol

   - making their rules known with respect to programmatic access
       - who is (not) allowed to scrape what, and sometimes, at what speed     
<br>       
   - within `robots.txt` files
       - available at, e.g., www.websiteurl.com/robots.txt

<br>

- The rules in `robots.txt` cannot not enforced
  - but should be respected for ethical reasons
  
<br>

- The language in `robots.txt` files is specific but intuitive
  - easy to read and understand
  - the `robotstxt` package makes it even easier

---
## Considerations &mdash; the Ethics &mdash; `robots.txt` &mdash; Syntax

.pull-left[

- It has pre-defined keys, most importantly

   - `User-agent` indicates who the protocol is for
   
   - `Allow` indicates which part(s) of the website can be scraped
   
   - `Disallow` indicates which part(s) must not be scraped
   
   - `Crawl-delay` indicates how fast the website could be scraped

<br>

- In case you write your own protocol one day, note that

   - the keys start with capital letters
   - they are followed by a colon .yellow-h[:]

]

.pull-right[

```md
`User-agent:`
`Allow:`
`Disallow:`
`Crawl-delay:`

```

]

---
## Considerations &mdash; the Ethics &mdash; `robots.txt` &mdash; Syntax

.pull-left[

- Websites define their own values

   - after the colon and a white space

<br>

- Note that

   - &#42; indicates the protocol is for everyone
   - `/` indicates all sections and pages
   - `/about/` indicates a specific path
   - values for `Crawl-delay` indicate seconds    
<br>   
   - this website allows anyone to scrape, provided that
       - `/about/` is left out, and 
       - the website is accessed at 5-seconds intervals

]

.pull-right[

```md
User-agent: `*`
Allow: `/`
Disallow: `/about/`
Crawl-delay: `5`

```

]

---
## Considerations &mdash; the Ethics &mdash; `robots.txt` &mdash; Syntax

Files might include optional comments, written after he number sign .yellow-h[&#x23;]


```md
`# thank you for respecting our protocol`

User-agent: *
Allow: /
Disallow: /about/
Crawl-delay: 5    `# five second delay, to ensure our servers are not overloaded`

```

---
## Considerations &mdash; the Ethics &mdash; `robots.txt` &mdash; Syntax

The protocol of this website only applies to Google

- Google is allowed to scrape everything
- there is no defined rule for anyone else

<br>

```md
User-agent: `googlebot`
Allow: /

```

---
## Considerations &mdash; the Ethics &mdash; `robots.txt` &mdash; Syntax


The protocol of this website only applies to Google

- Google is .yellow-h[disallowed] to scrape .yellow-h[two] specific paths
    - with no limit on speed
- there is no defined rule for anyone else

<br>

```md
User-agent: googlebot
`Disallow: /about/`
`Disallow: /history/`

```

---
## Considerations &mdash; the Ethics &mdash; `robots.txt` &mdash; Syntax


This website has different protocols for different agents

- Google is allowed to scrape everything, with a 5-second delay
- Bing is not allowed to scrape anything
- everyone else can scrape the section or page located at www.websiteurl/about/

<br>

```md
User-agent: googlebot
Allow: /
Crawl-delay: 5

User-agent: bing
Disallow: /

User-agent: *
Allow: /about/

```

---
## Considerations &mdash; the Ethics &mdash; `robotstxt`

- The `robotstxt` packages facilitates checking website protocols

   - from within R &mdash; no need to visit websites via browser
   - provides functions to check, among others, the rules for specific paths and/or agents

<br>

- There are two main functions

   - `robotstxt`, which gets complete protocols
   - `paths_allowed`, which checks protocols for one or more specific paths


---
## Considerations &mdash; the Ethics &mdash; `robotstxt`

.pull-left[

Use the `robotstxt` function to get a protocol
- supply a base url with the `domain` argument
   - as a string
   - probably the only argument that you will need

]


.pull-right[

```md

robotstxt(
  domain = NULL,
  ...
)

```

]

---
## Considerations &mdash; the Ethics &mdash; `robotstxt`

```md
library(robotstxt)
robotstxt(domain = "https://parliament-luzland.netlify.app")
```
<br>

.out-t[
...

User-agent: googlebot     
Disallow: /states/

User-agent: *     
Allow: /

$robexclobj

&#60;Robots Exclusion Protocol Object>

$bots
[1] "googlebot" "*"  

...
]

---
## Considerations &mdash; the Ethics &mdash; `robotstxt`

.pull-left[

Use the `paths_allowed` function to checks protocols for one or more specific paths
- supply a base url with the `domain` argument
- `path` and `bot` are the other important arguments
   - notice the default values    
<br>
- leads to either `TRUE` (allowed to scrape) or `FALSE` (not allowed)

]


.pull-right[

```md

paths_allowed(
  domain = "auto",
  paths = "/",
  bot = "*",
  ...
)
```

]

---
## Considerations &mdash; the Ethics &mdash; `robotstxt`

```md
paths_allowed(domain = "https://parliament-luzland.netlify.app")
```

.out-t[

[1] TRUE

]

```md
paths_allowed(domain = "https://parliament-luzland.netlify.app", 
              `paths = c("/states/", "/constituencies/")`)
```

.out-t[

[1] TRUE  TRUE

]

```md
paths_allowed(domain = "https://parliament-luzland.netlify.app", 
              paths = c("/states/", "/constituencies/"), `bot = "googlebot"`)
```

.out-t[

[1] FALSE  TRUE

]

---
class: action

## Exercises

1) Check the protocols for <https://www.theguardian.com>

   - via a browser and with the `robotstxt` function
   - compare what you see
   
<br>

2) Check a path with the `paths_allowed` function

   - such that it will return `FALSE`
   
<br>

3) Check the protocols for any website that you might wish to scrape

  - with the `robotstxt` function

`r countdown(minutes = 7, seconds = 30, top = 0)`

---
## Considerations &mdash; the Ethics &mdash; Speed

- Websites are designed for visitors with human-speed in mind

   - computer-speed visits can overload servers, depending on their bandwidth
      - popular websites might have more bandwidth
      - but, they might attract multiple scrapers at the same time

<br>

- Waiting a little between two visits makes scraping more ethical
    
  - waiting time may or may not be defined in the protocol
      - lookout for, and respect, the `Crawl-delay` key in `robots.txt`     
<br>
  - [Part 4](#part4) covers how to wait
  
<br>

- Not waiting enough might lead to a ban
    - by site owners, administrators
    - for IP addresses with undesirably high number of visits in a short period of time
      
---
## Considerations &mdash; the Ethics &mdash; Purpose

Ideally, we scrape for a purpose
  
- e.g., for academics, to answer one or more research questions, test hypotheses    
<br>
      - developed prior to data collection, analysis
          - based on, e.g., theory, claims, observations   
<br>
      - perhaps, even pre-registered
          - e.g., at [OSF Registries](https://osf.io/registries)

---
## Considerations &mdash; Data Storage

Scraped data frequently requires 

- large amounts of digital storage space
  - internet data is typically big data    
<br>
- private, safe storage spaces
  - due to local rules, institutional requirements

---
name: part3
class: inverse, center, middle

# Part 3. HTML Basics

.footnote[

[Back to the contents slide](#contents-slide).

]

---








---
name: part4
class: inverse, center, middle

# Part 3. Data Collection

.footnote[

[Back to the contents slide](#contents-slide).

]

---
name: part4
class: inverse, center, middle

# Part 4. Data Preperation

.footnote[

[Back to the contents slide](#contents-slide).

]

---
## Data Preperation &mdash; Overview

- The `rtweet` package does a very good job with data preperation to start with

   - returns data frames, with mostly tidy data
   - although Twitter APIs return nested lists
   - some variables are still lists
      - e.g., `hastags`

--

<br>

- Further data preparation depends on your research project

   - most importantly, on whether you will work with texts or not
   - we will cover some common preparation steps

---
## Data Preperation &mdash; Overview &mdash; Strings

- Most researchers would be interested in textual Twitter data

   - tweets as a whole, but also specifically hashtags *etc*.

--

<br>

- There are many components of tweets as texts

   - e.g., mentions, hashtags, emojis, links *etc*.
   - but also punctuation, white spaces, upper case letters *etc*.
   - some of these may need to be taken out before analysis

--

<br>

- I use the `stringr` package ([Wickham, 2019](https://cran.r-project.org/web/packages/stringr/index.html)) for string operations

   - part of the `tidyverse` family
   - you might have another favourite already
       - no need to change as long as it does the job
       
---
## Data Preperation &mdash; Overview &mdash; Numbers

- There is more to Twitter data than just tweets
      
  - e.g., the number of followers, likes *etc*.
     - see Silva and Proksch ([2020](https://doi.org/10.1017/S0003055420000817)) for a great example

--
<br>

- I use the `dplyr` package ([Wickham et al, 2020](https://cran.r-project.org/web/packages/dplyr/index.html) for most data operations

   - part of the `tidyverse` family
   - you might have another favourite already
       - no need to change as long as it does the job

---
## Data Preperation &mdash; Remove Mentions

```{r, eval=FALSE}
tweet <- "These from @handle1 are #socool. ðŸ‘ A #mustsee, @handle2! 
          ðŸ‘‰ t.co/aq7MJJ1
          ðŸ‘‰ https://t.co/aq7MJJ2"
````

```{r, eval=FALSE}
str_remove_all(string = tweet, pattern = "[@][\\w_-]+")
```


.out-t[

[1] "This from  are #socool. ðŸ‘ A #mustsee, ! 
          ðŸ‘‰ t.co/aq7MJJ1
          ðŸ‘‰ https://t.co/aq7MJJ2"

]


---
## Data Preperation &mdash; Remove Hashtags


```{r, eval=FALSE}
tweet <- "These from @handle1 are #socool. ðŸ‘ A #mustsee, @handle2! 
          ðŸ‘‰ t.co/aq7MJJ1
          ðŸ‘‰ https://t.co/aq7MJJ2"
````

```{r, eval=FALSE}
str_remove_all(string = tweet, pattern = "[`#`][\\w_-]+")
```


.out-t[

[1] "These from @handle1 are . ðŸ‘ A , @handle2! 
          ðŸ‘‰ t.co/aq7MJJ1
          ðŸ‘‰ https://t.co/aq7MJJ2"

]

---
## Data Preperation &mdash; Remove Links

```{r, eval=FALSE}
tweet <- "These from @handle1 are #socool. ðŸ‘ A #mustsee, @handle2! 
          ðŸ‘‰ t.co/aq7MJJ1
          ðŸ‘‰ https://t.co/aq7MJJ2"

````

```{r, eval=FALSE}
str_remove_all(string = tweet, pattern = "http\\S+\\s*")
```


.out-t[

[1] "These from @handle1 are. ðŸ‘ A, @handle2! 
          ðŸ‘‰ t.co/aq7MJJ1"

]

<br>

- Notice that

  - links come in various formats
  - you may need multiple or complicated regular expression patterns

---
## Data Preperation &mdash; Remove Links &mdash; Alternative

Use the `urls_t.co` variable to remove all links

- if there are more than one link in a tweet, they are stored as a list in this variable


```{r, eval=FALSE}
# start with your existing dataset of tweets
df_tweets <- df_tweets %>%
  
# limit the operation to within individual tweets  
  group_by(status_id) %>%
  
# create a new variable of tweets without links  
  mutate(tidy_text = 
           
# by removing them from the existing variable `text`           
           str_remove_all(string = text, 
 
# that matches the `urls_t.co` variable, after being collapsed into a string      
           pattern = str_c(unlist(urls_t.co), collapse = "|")))
```

--
class: action

`r countdown(minutes = 8, seconds = 00, top = 0)`

---
## Data Preperation &mdash; Remove Emojis

```{r, eval=FALSE}
tweet <- "These from @handle1 are #socool. ðŸ‘ A #mustsee, @handle2! 
          ðŸ‘‰ t.co/aq7MJJ1
          ðŸ‘‰ https://t.co/aq7MJJ2"
````

```{r, eval=FALSE}
iconv(x = tweet, from = "latin1", to = "ASCII", sub = "")
```


.out-t[

[1] "These from @handle1 are #socool. A #mustsee, @handle2! 
          t.co/aq7MJJ1
          https://t.co/aq7MJJ2"

]


---
## Data Preperation &mdash; Exercises &mdash; Notes


- The exercises in this part are best followed by

  - using `tweets.rds` or similar dataset
  - saving a new variable at every step of preparation
  - observing the newly created variables 
      - to confirm whether the code works as intended

<br>

- The `mutate` function, from the `dplyr` package, can be helpful, as follows

   - recall that `text` is the variable for tweets

```{r, eval=FALSE}

tweets <- read_rds("data/tweets.rds")

clean_tweets <- tweets %>%
  `mutate(no_mentions` = str_remove_all(string = `text`, pattern = "[@][\\w_-]+"))

```

---
class: action

## Exercises


41) Remove mentions

- hint: the pattern is `"[@][\\w_-]+"`

42) Remove hastags
- hint: the pattern is `"[#][\\w_-]+"`

43) Remove links
- by using the links from the `urls_t.co` variable

44) Remove emojis
- pull the help file for the `iconv` function first


`r countdown(minutes = 10, seconds = 00, top = 0)`


---
## Data Preperation &mdash; Remove Punctuations

```{r, eval=FALSE}
tweet <- "These from @handle1 are #socool. ðŸ‘ A #mustsee, @handle2! 
          ðŸ‘‰ t.co/aq7MJJ1
          ðŸ‘‰ https://t.co/aq7MJJ2"
````

```{r, eval=FALSE}
str_remove_all(string = tweet, pattern = "[[:punct:]]")
```


.out-t[

[1] "This from are socool ðŸ‘ A mustsee handle2
          ðŸ‘‰ tcoaq7MJJ1
          ðŸ‘‰ httpst.coaq7MJJ2"

]

Notice that
   - this removed all punctuation, including those in mentions, hashtags, and links
   - if tweets are typed with no spaces after punctuation, this might lead to merged pieces of text
      - alternatively, try the `str_replace` function to replace punctuation with space
      
---
## Data Preperation &mdash; Remove Punctuations &mdash; Alternative

```{r, eval=FALSE}
tweet <- "This is a sentence.There is no space before this sentence."
````

```{r, eval=FALSE}
str_`remove`_all(string = tweet, pattern = "[[:punct:]]")
```

.out-t[

[1] "This is a sentenceThere is no space before this sentence"

]

--

<br>


```{r, eval=FALSE}
str_`replace`_all(string = tweet, pattern = "[[:punct:]]", replacement = " ")
```

.out-t[

[1] "This is a sentence There is no space before this sentence "

]

---
## Data Preperation &mdash; Remove Punctuations &mdash; Alternative


```{r, eval=FALSE}
tweet <- "This is a sentence.There is no space before this sentence."
````

```{r, eval=FALSE}
str_replace_all(string = tweet, pattern = "[[:punct:]]", replacement = " ")
```

.out-t[

[1] "This is a sentence There is no space before this sentence "

]

---
## Data Preperation &mdash; Remove Repeated Whitespace


```{r, eval=FALSE}
tweet <- "There are too many spaces after this sentence.   This is a new sentence."
````

```{r, eval=FALSE}
str_squish(string = tweet)
```

.out-t[

[1] "There are too many spaces after this sentence. This is a new sentence."

]

Note that
- white spaces can be introduced not only by users on Twitter, but also by us, while cleaning the data
   - e.g., removing and/or replacing operations above
   - hence, this function might be useful after other operations

---
## Data Preperation &mdash; Change Case

```{r, eval=FALSE}
tweet <- "lower case. Sentence case. Title Case. UPPER CASE."
````

```{r, eval=FALSE}
str_to_lower(string = tweet)
```

.out-t[

[1] "lower case. sentence case. title case. upper case."

]

Note that
   - there are other functions in this family, including
      - `str_to_sentence`, `str_to_title`, `str_to_upper`


---
class: action

## Exercises

45) Remove punctuations
- by using the `str_replace_all` function
- hint: the pattern is `[[:punct:]]`

<br>

46) Remove whitespace
- hint: the function is called `str_squish`

<br>

47) Change case to lower case
- hint: the function is called `str_to_lower`


`r countdown(minutes = 10, seconds = 00, top = 0)`

---
## Data Preperation &mdash; Change Unit of Observation

Research designs might require changing the unit of observation
   - aggregation
      - e.g., at the level of users, locations, hashtags etc.
      - summarise with `dplyr`
   

  - dis-aggregation
     - e.g., to the level of words
     - tokenise with `tidytext`
     
---
## Data Preperation &mdash; Change Unit of Observation &mdash; Aggregation

Aggregate at the level of users

- the number of tweets per user

```{r, eval=FALSE}
# load the tweets dataset
df <- read_rds("tweets.rds") %>%
  
# group by users for aggregation  
  group_by(user_id) %>%
  
# create summary statistics for variables of interest
  summarise(sum_tweets = n())
```

---
## Data Preperation &mdash; Change Unit of Observation &mdash; Aggregation

What is aggregated at which level depends on your research design, such as
   - aggregate the tweets into a single text
   - at the level of users by source


```{r, eval=FALSE}
# load the tweets dataset
df <- read_rds("tweets.rds") %>%
  
# group by users for aggregation  
  group_by(user_id, `source`) %>%
  
# create summary statistics for variables of interest
  summarise(`merged_tweets = paste0(text, collapse = ". ")`) 
```

---
## Data Preperation &mdash; Change Unit of Observation &mdash; Dis-aggregation

Disaggregate the tweets, by splitting them into smaller units
  - also called .yellow-h[tokenisation]

Note that
- by default `sep = "[^[:alnum:].]+"`, which works well with separating tweets into words
   - change this argument with a regular expression of your choice
- this creates a tidy dataset, where each observation is a word
   - all other tweet-level variables are repeated for each observation

```{r, eval=FALSE}
# load the tweets dataset
df <- read_rds("tweets.rds") %>%
  
# split the variable `text`  
    separate_rows(text)
```

---
## Data Preperation &mdash; Change Unit of Observation &mdash; Dis-aggregation

The `tidytext` has a function that works better with tokenising tweets
- with `token = "tweets"`, it dis-aggregates text into words
   - except that it respects usernames, hashtags, and URLS 

```{r, eval=FALSE}
# load the tweets dataset
df <- read_rds("tweets.rds") %>%
  
# split the variable `text`, create a new variable called `da_tweets`    
    unnest_tokens(output = da_tweets, input = text, token = "tweets")
```

---
## Data Preperation &mdash; Change Unit of Observation &mdash; Dis-aggregation

Tokenise variables to levels other than words
- e.g., characters, words (the default),  sentences, lines

```{r, eval=FALSE}
# load the tweets dataset
df <- read_rds("tweets.rds") %>%
  
# split the variable `text` into sentences, create a new variable called `da_tweets`    
    unnest_tokens(output = da_tweets, input = text, `token = "sentences"`)
```

---
## Data Preperation &mdash; Change Unit of Observation &mdash; Dis-aggregation

Tokenise variables other than tweets
- recall that `rtweet` stores multiple hastags, mentions *etc*. as lists


```{r, eval=FALSE}
# load the tweets dataset
df <- read_rds("tweets.rds") %>%
  
# unlist the lists of hashtags to create strings  
  group_by(status_id) %>%
  mutate(tidy_hashtags = str_c(unlist(hashtags), collapse = " ")) %>%
  
# split the string, create a new variable called `da_tweets`    
  unnest_tokens(output = da_hashtags, input = tidy_hashtags, token = "words")
```

---
## Data Preperation &mdash; Remove Stop Words

Remove the common, uninformative words
- e.g., the, a, i

Note that 
- this operation requires a tokenised-to-word variable
- stop words for English are stored in the `stop_words` dataset in the `tidytext` variable
- list of words for other languages are available elsewhere, including 
   - the `stopwordslangs` function from the `rtweet` package
   - the `stopwords` function from the `tm` package
      - e.g., use `tm::stopwords("german")` for German

```{r, eval=FALSE}
# load the tweets dataset
df <- read_rds("tweets.rds") %>%
  
# split the variable `text`, create a new variable called `da_tweets`    
  unnest_tokens(output = da_tweets, input = text, token = "tweets") %>%
 
# remove rows that match any of the stop words as stored in the stop_words dataset 
  anti_join(stop_words, by = c("da_tweets" = "word"))
```

---
class: action

## Exercises

48) Aggregate `text` to a higher level
- e.g., if you are not using `tweets.rds`, to MP level
   - if not, perhaps to `source` level

<br>

49) Dis-aggregate `text` to a lower level
- e.g., to words

<br>

50) Dis-aggregate `hashtags`
- i.e., make sure each row has at most one hashtag

<br>

51) Remove stop words


`r countdown(minutes = 10, seconds = 00, top = 0)`

---
name: reference-slide
class: inverse, center, middle

# References

.footnote[

[Back to the contents slide](#contents-slide).

]

---
## References

Harrison, J. (2020). [RSelenium: R Bindings for 'Selenium WebDriver'](https://cran.r-project.org/web/packages/RSelenium/index.html). R package, version 1.7.7.

Meissner, P., & Ren, K. (2020). [robotstxt: A 'robots.txt' Parser and 'Webbot'/'Spider'/'Crawler' Permissions Checker](https://cran.r-project.org/web/packages/robotstxt/index.html). R package, version 0.7.13.

Perepolkin, D. (2019). [polite: Be Nice on the Web](https://cran.r-project.org/web/packages/polite/index.html). R package, version 0.1.1.

Robinson, D., & Silge, J. (2021). [tidytext: Text mining using 'dplyr', 'ggplot2', and other tidy tools](https://cran.r-project.org/web/packages/tidytext/index.html). R package, version 0.3.0.

Silge, J., & Robinson, D. (2017). [Text mining with R: A tidy approach](https://www.tidytextmining.com). O'Reilly. Open access at

Wickham, H. (2019). [stringr: Simple, Consistent Wrappers for Common String Operations](https://cran.r-project.org/web/packages/stringr/index.html). R package, version 1.4.0.

Wickham, H. (2021). [rvest: Easily Harvest (Scrape) Web Pages](https://cran.r-project.org/web/packages/rvest/index.html). R package, version 1.0.0.




Wickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., Woo, K., Yutani, H. and Dunnington, D. (2020). [dplyr: A grammar of data manipulation](https://cran.r-project.org/web/packages/dplyr/index.html). R package, version 0.8.5.

Wickham, H. and Grolemund, G. (2019). [R for data science](https://r4ds.had.co.nz). O'Reilly. Open access at [https://r4ds.had.co.nz](https://r4ds.had.co.nz/).

Wickham, H., RStudio (2019). [https://cran.r-project.org/web/packages/tidyverse/index.html](tidyverse: Easily Install and Load the 'Tidyverse'). R package, version 3.3.3.

Xie, Y. (2020). [xaringan: Presentation Ninja](https://cran.r-project.org/web/packages/xaringan/index.html). R package, version 0.19.

---
class: middle, center

## The workshop ends here.
## Congradulations for making it this far, and
## thank you for joining me!

.footnote[

[Back to the contents slide](#contents-slide).

]

```{r, print, eval=FALSE, echo=FALSE, message=FALSE}
pagedown::chrome_print("scrp_workshop.html", output = "scrp_workshop.pdf")
```