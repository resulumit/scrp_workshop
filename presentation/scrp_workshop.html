<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Automated Web Scraping with R</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link href="libs/countdown/countdown.css" rel="stylesheet" />
    <script src="libs/countdown/countdown.js"></script>
  </head>
  <body>
    <textarea id="source">


class: inverse, center, middle



&lt;style type="text/css"&gt;

.hljs-github .hljs {
    background: #e5e5e5;
}

.inline-c, remark-inline-code {
   background: #e5e5e5;
   border-radius: 3px;
   padding: 4px;
   font-family: 'Source Code Pro', 'Lucida Console', Monaco, monospace;
}


.yellow-h{
   background: #ffff88;
}


.out-t, remark-inline-code {
   background: #9fff9f;
   border-radius: 3px;
   padding: 4px;
   
}

.pull-left-c {
    float: left;
    width: 58%;
    
}

.pull-right-c {
    float: right;
    width: 38%;
    
}

.medium {
    font-size: 75%
    
}

.small {
    font-size: 50%
    }

.action {
    background-color: #f2eecb;
  
}

.remark-code {
  display: block;
  overflow-x: auto;
  padding: .5em;
  color: #333;
  background: #9fff9f;
}


&lt;/style&gt;


# Automated Web Scraping with R

&lt;br&gt;

### Resul Umit

### May 2021

.footnote[

[Skip intro &amp;mdash; To the contents slide](#contents-slide).
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="mailto:resuluy@uio.no?subject=Twitter workshop"&gt;I can teach this workshop at your institution &amp;mdash; Email me&lt;/a&gt;.

]

---
## Who am I?

Resul Umit

- post-doctoral researcher in political science at the University of Oslo

- teaching and studying representation, elections, and parliaments
    - [a recent publication](https://doi.org/10.1177%2F1478929920967588):
    Parliamentary communication allowances do not increase electoral turnout or incumbents‚Äô vote share

--

&lt;br&gt;

- teaching workshops, also on

  - [writing reproducible research papers](https://resulumit.com/blog/rmd-workshop/)
  - [version control and collaboration](https://resulumit.com/teaching/git_workshop.html)
  - [working with Twitter data](https://resulumit.com/teaching/twtr_workshop.html)
  - [creating academic websites](https://resulumit.com/teaching/rbd_workshop.html)
    
--

&lt;br&gt;

- more information available at [resulumit.com](https://resulumit.com/)

---
## The Workshop &amp;mdash; Overview

- One day, on how to automate the process of extracting data from websites

  - 200+ slides, 75+ exercises
  - a [demonstration website](https://parliament-luzland.netlify.app/) for practice

--

&lt;br&gt;

- Designed for researchers with basic knowledge of R programming language

  - does not cover programming with R
      - e.g., we will use existing functions and packages   
&lt;br&gt;
  - ability to work with R will be very helpful
      - but not absolutely necessary &amp;mdash; this ability can be developed during and after the workshop as well
        
---
## The Workshop &amp;mdash; Motivation

- Data available on websites provide attractive opportunities for academic research

  - e.g., parliamentary websites were the main source of data for my PhD

--

&lt;br&gt;

- Acquiring such data requires 

  - either a lot of resources, such as time
  - or a set of skills, such as automated web scraping

--

&lt;br&gt;

- Typically, such skills are not part of academic training

  - for my PhD, I hand-visited close to 3000 webpages to collect data manually
      - on members of ten parliaments
      - multiple times, to update the dataset as needed

---
## The Workshop &amp;mdash; Motivation &amp;mdash; Aims

- To provide you with an understanding of what is ethically possible

  - we will cover a large breath of issues, not all of it is for long-term memory
      - hence the slides are designed for self study as well    
&lt;br&gt;
  - awareness of what is ethical and possible, `Google`, and perseverance are all you need

--

&lt;br&gt;

- To start you with acquiring and practicing the skills needed 

  - practice with the demonstration website
     - plenty of data, stable structure, and an ethical playground
  - start working on a real project

---
name: contents-slide

## The Workshop &amp;mdash; Contents

&lt;br&gt;

.pull-left[

[Part 1. Getting the Tools Ready](#part1)
   - e.g., installing software
   
[Part 2. Preliminary Considerations](#part2)
   - e.g., ethics of web scraping

[Part 3. Webpage Source Code](#part3)
   - e.g., elements and selectors

]

.pull-right[
   

[Part 4. Scraping Static Pages](#part4)
   - e.g., getting text from an element
   - by using `rvest`
  
[Part 5. Scraping Dynamic Pages](#part5)
   - e.g., clicking before scraping
   - by using `RSelenium` and `rvest`
   
]

.footnote[

[To the list of references](#reference-slide).

] 

---
## The Workshop &amp;mdash; Organisation

- ~~Sit in groups of two~~ Breakout in groups of two for exercises

  - participants learn as much from their partner as from instructors
  - switch partners after every other part
  - leave your breakout room manually, when everyone in the group is ready

&lt;br&gt; 

- Type, rather than copy and paste, the code that you will find on these slides

  - typing is a part of the learning process
  - slides are, and will remain, available at [resulumit.com/teaching/scrp_workshop.html](https://resulumit.com/teaching/scrp_workshop.html)

&lt;br&gt; 

- When you have a question

  - ask your partner
  - google together
  - ask me

---
class: action

## The Workshop &amp;mdash; Organisation &amp;mdash; Slides

Slides with this background colour indicate that your action is required, for

- setting the workshop up
    - e.g., installing R 
    
- completing the exercises
    - e.g., downloading tweets
    - there are 75+ exercises
    - these slides have countdown timers
        - as a guide, not to be followed strictly
    
<div class="countdown" id="timer_60a7876c" style="top:0;right:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---
## The Workshop &amp;mdash; Organisation &amp;mdash; Slides

- Codes and texts that go in R console or scripts .inline-c[appear as such &amp;mdash; in a different font, on gray background]    

    - long codes and texts will have their own line(s)


```r
# read in  the tweets dataset
df &lt;- read_rds("tweets.rds") %&gt;%
  
# split the variable text, create a new variable called da_tweets   
  unnest_tokens(output = da_tweets, input = text, token = "tweets") %&gt;%
 
# remove rows that match any of the stop words as stored in the stop_words dataset 
  anti_join(stop_words, by = c("da_tweets" = "word")) 
```

---
## The Workshop &amp;mdash; Organisation &amp;mdash; Slides

- Codes and texts that go in R console or scripts .inline-c[appear as such &amp;mdash; in a different font, on gray background]    

    - long codes and texts will have their own line(s)

&lt;br&gt;

- Results that come out as output .out-t[appear as such &amp;mdash; in the same font, on green background]    

    - except very obvious results, such as figures and tables
    
--

&lt;br&gt;

    
- Specific sections are .yellow-h[highlighted yellow as such] for emphasis

    - these could be for anything &amp;mdash; codes and texts in input, results in output, and/or texts on slides
    
--

&lt;br&gt;

- The slides are designed for self-study as much as for the workshop

    - *accessible*, in substance and form, to go through on your own

---
name: part1
class: inverse, center, middle

# Part 1. Getting the Tools Ready

.footnote[

[Back to the contents slide](#contents-slide).

]

---
class: action

## Workshop Slides &amp;mdash; Access on Your Browser

- Having the workshop slides&lt;sup&gt;*&lt;/sup&gt; on your own machine might be helpful

  - flexibility to go back and forward on your own
      - especially while in a breakout room
  - ability to scroll across long codes on some slides

&lt;br&gt;

- Access at &lt;https://resulumit.com/teaching/scrp_workshop.html&gt;

  - will remain accessible after the workshop
  - might crash for some Safari users
      - if using a different browser application is not an option, view the [PDF version of the slides](https://github.com/resulumit/twtr_workshop/blob/master/presentation/twtr_workshop.pdf) on GitHub
  
.footnote[

&lt;sup&gt;*&lt;/sup&gt; These slides are produced in R, with the `xaringan` package ([Xie, 2020](https://cran.r-project.org/web/packages/xaringan/xaringan.pdf)).

]

---
class: action

## Demonstration Website &amp;mdash; Explore on Your Browser

- There is a demonstration website for this workshop

   - available at &lt;https://parliament-luzland.netlify.app/&gt;
   - includes fabricated data on the imaginary Parliament of Luzland
   - provides us with plenty of data, stable structure, and an ethical playground

&lt;br&gt;

- Using this demonstration website for practice is recommended

   - tailored to exercises, no ethical concern
   - but not compulsory &amp;mdash; use a different one if you prefer so

&lt;br&gt;
 
- Explore the website now
   - see the four sections
   - click on the links to see an individual page for 
       - states, constituencies, members, and documents

<div class="countdown" id="timer_60a7869f" style="top:0;right:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">05</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>
   
---
class: action

## R &amp;mdash; Download from the Internet and Install

- Programming language of this workshop

  - created for data analysis, extending for other purposes
      - e.g., accessing websites
  - allows for all three steps in one environment
      - accessing websites; scraping and processing data
  - an alternative: [python](https://www.python.org/)

&lt;br&gt;

- Download R from [https://cloud.r-project.org](https://cloud.r-project.org)

  - optional, if you have it already installed &amp;mdash; but then consider updating&lt;sup&gt;*&lt;/sup&gt;
      - the `R.version.string` command checks the version of your copy
      - compare with the latest official release at [https://cran.r-project.org/sources.html](https://cran.r-project.org/sources.html)



.footnote[

&lt;sup&gt;*&lt;/sup&gt; The same applies to all software that follows &amp;mdash; consider updating if you have them already installed. This ensures everyone works with the latest, exactly the same, tools.

]


---
class: action

## RStudio &amp;mdash; Download from the Internet and Install

- Optional, but highly recommended

  - facilitates working with R

&lt;br&gt;

- A popular integrated development environment (IDE) for R

  - an alternative: [GNU Emacs](https://www.gnu.org/software/emacs/)

&lt;br&gt;

- Download RStudio from [https://rstudio.com/products/rstudio/download](https://rstudio.com/products/rstudio/download)

  - choose the free version
  - to check for any updates, follow from the RStudio menu:

&gt; `Help -&gt; Check for Updates`

---
class: action
name: rstudio-project

## RStudio Project &amp;mdash; Create from within RStudio 

- RStudio allows for dividing your work with R into separate projects

  - each project gets dedicated workspace, history, and source documents
  - [this page](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects) has more information on why projects are recommended

&lt;br&gt;

- Create a new RStudio project for for this workshop, following from the RStudio menu:

&gt; `File -&gt; New Project -&gt; New Directory -&gt; New Project`

&lt;br&gt;

- Choose a location for the project with `Browse...`

  - avoid choosing a synced location, e.g., `Dropbox`
      - likely to cause warning and/or error messages
      - if you must, pause syncing, or add an sync exclusion

---
class: action

## R Packages &amp;mdash; Install from within RStudio&lt;sup&gt;*&lt;/sup&gt;

Install the packages that we need


```r
install.packages(c("rvest", "RSelenium", "robotstxt", "polite",
                   "tidyverse", "tidytext"))
```


.footnote[

&lt;sup&gt;*&lt;/sup&gt; You may already have a copy of one or more of these packages. In that case, I recommend updating by re-installing them now.

]

<div class="countdown" id="timer_60a7866f" style="top:0;right:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">02</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---
class: action

## R Packages &amp;mdash; Install from within RStudio

Install the packages that we need


```r
install.packages(c("rvest", "RSelenium", "robotstxt", "polite",
                   "tidyverse", "tidytext"))
```

&lt;br&gt;

We will use

- `rvest` ([Wickham, 2021](https://cran.r-project.org/web/packages/rvest/index.html)), for scraping websites

--
- `RSelenium` ([Harrison, 2020](https://cran.r-project.org/web/packages/RSelenium/index.html)), for browsing the web programmatically

--
- `robotstxt` ([Meissner &amp; Ren, 2020](https://cran.r-project.org/web/packages/robotstxt/index.html)), for checking permissions to scrape websites

--
- `polite` ([Perepolkin, 2019](https://cran.r-project.org/web/packages/polite/index.html)), for compliance with permissions to scrape websites

---
class: action

## R Packages &amp;mdash; Install from within RStudio


```r
install.packages(c("rvest", "RSelenium", "robotstxt", "polite",
                   "tidyverse", "tidytext"))
```

- `tidyverse` ([Wickham &amp; RStudio 2019](https://cran.r-project.org/web/packages/tidyverse/index.html)), for various tasks

    - including data manipulation, visualisation
    - alternative: e.g., `base` R

--
&lt;br&gt;

- `tidytext` ([Robinson &amp; Silge, 2021](https://cran.r-project.org/web/packages/tidytext/index.html)), for working with text as data


---
class: action

## R Script &amp;mdash; Start Your Script

.pull-left[

- Check that you are in the newly created project
    - indicated at the upper-right corner of RStudio window

- Create a new R Script, following from the RStudio menu

&gt; `File -&gt; New File -&gt; R Script`

- Name and save your file
    - to avoid the `Untitled123` problem
    - e.g., `scrape_web.R`


- Load the `rvest` and other packages

]

.pull-right[


```r
library(rvest)
library(RSelenium)
library(robotstxt)
library(polite)
library(tidyverse)
library(tidytext)
```

]

---
class: action

## Java &amp;mdash; Download from the Internet and Install


- A language and software that `RSelenium` needs
   - for automation scripts

&lt;br&gt;

- Download Java from &lt;https://www.java.com/en/download/&gt;
   - requires restarting any browser that you might have open
   
---
class: action

## Chrome &amp;mdash; Download from the Internet and Install


- A browser that facilitates web scraping
   - favoured by `RSelenium` and most programmers

&lt;br&gt;

- Download Chrome from &lt;https://www.google.com/chrome/&gt;

---
class: action

## ScrapeMate &amp;mdash; Add Extension to Browser 

- An [open source software](https://github.com/hermit-crab/ScrapeMate) extension to Chrome, Firefox  

   - facilitates selecting what to scrape from a webpage
   - optional, but highly recommended

&lt;br&gt;

- Add the extension to your preferred browser
   
  - for Chrome, search at &lt;https://chrome.google.com/webstore/category/extensions&gt;
  - for Firefox, search at &lt;https://addons.mozilla.org/&gt;

&lt;br&gt;

- If you cannot use Chrome or Firefox
   - drag and drop the following link to your bookmarks bar: &lt;a href="javascript:(function(){var%20s=document.createElement('div');s.innerHTML='Loading...';s.style.color='black';s.style.padding='20px';s.style.position='fixed';s.style.zIndex='9999';s.style.fontSize='3.0em';s.style.border='2px%20solid%20black';s.style.right='40px';s.style.top='40px';s.setAttribute('class','selector_gadget_loading');s.style.background='white';document.body.appendChild(s);s=document.createElement('script');s.setAttribute('type','text/javascript');s.setAttribute('src','https://dv0akt2986vzh.cloudfront.net/unstable/lib/selectorgadget.js');document.body.appendChild(s);})();"&gt;SelectorGadget&lt;/a&gt;
       - another &amp;mdash; similar but older &amp;mdash; [open source software](https://github.com/cantino/selectorgadget) with the same functionality


---
## Other Resources&lt;sup&gt;*&lt;/sup&gt;

- `RSelenium` vignettes

   - available at &lt;https://cran.r-project.org/web/packages/RSelenium/vignettes/basics.html&gt;

--

&lt;br&gt;

- R for Data Science ([Wickham &amp; Grolemund, 2019](https://r4ds.had.co.nz/))
  
   - open access at &lt;https://r4ds.had.co.nz&gt;
   
--

&lt;br&gt;

- Text Mining with R: A Tidy Approach ([Silge &amp; Robinson, 2017](https://www.tidytextmining.com/))
 
   - open access at [tidytextmining.com](https://www.tidytextmining.com/)
   - comes with [a course website](https://juliasilge.shinyapps.io/learntidytext/) where you can practice
   

   
.footnote[

&lt;sup&gt;*&lt;/sup&gt; I recommend these to be consulted not during but after the workshop.

]

---
name: part2
class: inverse, center, middle

# Part 2. Preliminary Considerations

.footnote[

[Back to the contents slide](#contents-slide).

]

---
## Considerations &amp;mdash; the Law

- Web scraping might be illegal     
&lt;br&gt;
   - depending on who is scraping what, why, how &amp;mdash; and under which jurisdiction
   - reflect, and check, before you scrape

--

&lt;br&gt;

- Web scraping might be more likely to be illegal if, for example,      
&lt;br&gt;
   - it is harmful to the source
      - commercially
          - e.g., scraping a commercial website to create a rival website     
      - physically
          - e.g., scraping a website so hard and fast that it collapses    
&lt;br&gt;
   - it gathers data that is
      - under copyright
      - not meant for the public to see
      - then used for financial gain

---
## Considerations &amp;mdash; the Ethics

- Web scraping might be unethical
   - even when it is legal
   - depending on who is scraping what, why, and how
   - reflect before you scrape

--

&lt;br&gt;

- Web scraping might be more likely to be unethical if, for example,    
&lt;br&gt;
   - it is &amp;mdash; edging towards &amp;mdash; illegal
   - it does not respect the restrictions
      - as defined in `robots.txt` files    
&lt;br&gt;
   - it harvests data 
       - that is otherwise available to download, e.g., through APIs
       - without purpose, at dangerous speed, repeatedly


---
## Considerations &amp;mdash; the Ethics &amp;mdash; `robots.txt`

- Most websites declare a robots exclusion protocol

   - making their rules known with respect to programmatic access
       - who is (not) allowed to scrape what, and sometimes, at what speed     
&lt;br&gt;       
   - within `robots.txt` files
       - available at, e.g., www.websiteurl.com/robots.txt

&lt;br&gt;

- The rules in `robots.txt` cannot not enforced
  - but should be respected for ethical reasons
  
&lt;br&gt;

- The language in `robots.txt` files is specific but intuitive
  - easy to read and understand
  - the `robotstxt` package makes it even easier

---
## Considerations &amp;mdash; the Ethics &amp;mdash; `robots.txt` &amp;mdash; Syntax

.pull-left[

- It has pre-defined keys, most importantly

   - `User-agent` indicates who the protocol is for
   
   - `Allow` indicates which part(s) of the website can be scraped
   
   - `Disallow` indicates which part(s) must not be scraped
   
   - `Crawl-delay` indicates how fast the website could be scraped

&lt;br&gt;

- In case you write your own protocol one day, note that

   - the keys start with capital letters
   - they are followed by a colon .yellow-h[:]

]

.pull-right[

```md
`User-agent:`
`Allow:`
`Disallow:`
`Crawl-delay:`

```

]

---
## Considerations &amp;mdash; the Ethics &amp;mdash; `robots.txt` &amp;mdash; Syntax

.pull-left[

- Websites define their own values

   - after the colon and a white space

&lt;br&gt;

- Note that

   - &amp;#42; indicates the protocol is for everyone
   - `/` indicates all sections and pages
   - `/about/` indicates a specific path
   - values for `Crawl-delay` indicate seconds    
&lt;br&gt;   
   - this website allows anyone to scrape, provided that
       - `/about/` is left out, and 
       - the website is accessed at 5-seconds intervals

]

.pull-right[

```md
User-agent: `*`
Allow: `/`
Disallow: `/about/`
Crawl-delay: `5`

```

]

---
## Considerations &amp;mdash; the Ethics &amp;mdash; `robots.txt` &amp;mdash; Syntax

Files might include optional comments, written after he number sign .yellow-h[&amp;#x23;]


```md
`# thank you for respecting our protocol`

User-agent: *
Allow: /
Disallow: /about/
Crawl-delay: 5    `# five second delay, to ensure our servers are not overloaded`

```

---
## Considerations &amp;mdash; the Ethics &amp;mdash; `robots.txt` &amp;mdash; Syntax

The protocol of this website only applies to Google

- Google is allowed to scrape everything
- there is no defined rule for anyone else

&lt;br&gt;

```md
User-agent: `googlebot`
Allow: /

```

---
## Considerations &amp;mdash; the Ethics &amp;mdash; `robots.txt` &amp;mdash; Syntax


The protocol of this website only applies to Google

- Google is .yellow-h[disallowed] to scrape .yellow-h[two] specific paths
    - with no limit on speed
- there is no defined rule for anyone else

&lt;br&gt;

```md
User-agent: googlebot
`Disallow: /about/`
`Disallow: /history/`

```

---
## Considerations &amp;mdash; the Ethics &amp;mdash; `robots.txt` &amp;mdash; Syntax


This website has different protocols for different agents

- Google is allowed to scrape everything, with a 5-second delay
- Bing is not allowed to scrape anything
- everyone else can scrape the section or page located at www.websiteurl/about/

&lt;br&gt;

```md
User-agent: googlebot
Allow: /
Crawl-delay: 5

User-agent: bing
Disallow: /

User-agent: *
Allow: /about/

```

---
## Considerations &amp;mdash; the Ethics &amp;mdash; `robotstxt`

- The `robotstxt` packages facilitates checking website protocols

   - from within R &amp;mdash; no need to visit websites via browser
   - provides functions to check, among others, the rules for specific paths and/or agents

&lt;br&gt;

- There are two main functions

   - `robotstxt`, which gets complete protocols
   - `paths_allowed`, which checks protocols for one or more specific paths


---
## Considerations &amp;mdash; the Ethics &amp;mdash; `robotstxt`

.pull-left[

Use the `robotstxt` function to get a protocol
- supply a base url with the `domain` argument
   - as a string
   - probably the only argument that you will need

]


.pull-right[

```md

robotstxt(
  domain = NULL,
  ...
)

```

]

---
## Considerations &amp;mdash; the Ethics &amp;mdash; `robotstxt`


```r
robotstxt(domain = "https://parliament-luzland.netlify.app")
```

```
## $domain
## [1] "https://parliament-luzland.netlify.app"
## 
## $text
## [robots.txt]
## --------------------------------------
## 
## User-agent: googlebot
## Disallow: /states/
## 
## User-agent: *
## Allow: /
## 
## 
## 
## 
## 
## $robexclobj
## &lt;Robots Exclusion Protocol Object&gt;
## $bots
## [1] "googlebot" "*"        
## 
## $comments
## [1] line    comment
## &lt;0 rows&gt; (or 0-length row.names)
## 
## $permissions
##      field useragent    value
## 1 Disallow googlebot /states/
## 2    Allow         *        /
## 
## $crawl_delay
## [1] field     useragent value    
## &lt;0 rows&gt; (or 0-length row.names)
## 
## $host
## [1] field     useragent value    
## &lt;0 rows&gt; (or 0-length row.names)
## 
## $sitemap
## [1] field     useragent value    
## &lt;0 rows&gt; (or 0-length row.names)
## 
## $other
## [1] field     useragent value    
## &lt;0 rows&gt; (or 0-length row.names)
## 
## $check
## function (paths = "/", bot = "*") 
## {
##     spiderbar::can_fetch(obj = self$robexclobj, path = paths, 
##         user_agent = bot)
## }
## &lt;bytecode: 0x0000020abf2f9c20&gt;
## &lt;environment: 0x0000020abdfe2d68&gt;
## 
## attr(,"class")
## [1] "robotstxt"
```

---
## Considerations &amp;mdash; the Ethics &amp;mdash; `robotstxt`

.pull-left[

Use the `paths_allowed` function to checks protocols for one or more specific paths
- supply a base url with the `domain` argument
- `path` and `bot` are the other important arguments
   - notice the default values    
&lt;br&gt;
- leads to either `TRUE` (allowed to scrape) or `FALSE` (not allowed)

]


.pull-right[

```md

paths_allowed(
  domain = "auto",
  paths = "/",
  bot = "*",
  ...
)
```

]

---
## Considerations &amp;mdash; the Ethics &amp;mdash; `robotstxt`


```r
paths_allowed(domain = "https://parliament-luzland.netlify.app")
```

```
## [1] TRUE
```


```md
paths_allowed(domain = "https://parliament-luzland.netlify.app", 
              `paths = c("/states/", "/constituencies/")`)
```


```
## [1] TRUE TRUE
```

```md
paths_allowed(domain = "https://parliament-luzland.netlify.app", 
              paths = c("/states/", "/constituencies/"), `bot = "googlebot"`)
```


```
## [1] FALSE  TRUE
```

---
class: action

## Exercises

1) Check the protocols for &lt;https://www.theguardian.com&gt;

   - via a browser and with the `robotstxt` function
   - compare what you see
   
&lt;br&gt;

2) Check a path with the `paths_allowed` function

   - such that it will return `FALSE`
   - taking the information from Exercise 1 into account
   
&lt;br&gt;

3) Check the protocols for any website that you might wish to scrape

  - with the `robotstxt` function

<div class="countdown" id="timer_60a786ba" style="top:0;right:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">07</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">30</span></code>
</div>

---
## Considerations &amp;mdash; the Ethics &amp;mdash; Speed

- Websites are designed for visitors with human-speed in mind

   - computer-speed visits can overload servers, depending on their bandwidth
      - popular websites might have more bandwidth
      - but, they might attract multiple scrapers at the same time

&lt;br&gt;

- Waiting a little between two visits makes scraping more ethical
    
  - waiting time may or may not be defined in the protocol
      - lookout for, and respect, the `Crawl-delay` key in `robots.txt`     
&lt;br&gt;
  - [Part 4](#part4) covers how to wait
  
&lt;br&gt;

- Not waiting enough might lead to a ban
    - by site owners, administrators
    - for IP addresses with undesirably high number of visits in a short period of time
      
---
## Considerations &amp;mdash; the Ethics &amp;mdash; Purpose

Ideally, we scrape for a purpose
  
- e.g., for academics, to answer one or more research questions, test hypotheses    
&lt;br&gt;
      - developed prior to data collection, analysis
          - based on, e.g., theory, claims, observations   
&lt;br&gt;
      - perhaps, even pre-registered
          - e.g., at [OSF Registries](https://osf.io/registries)

---
## Considerations &amp;mdash; Data Storage

Scraped data frequently requires 

- large amounts of digital storage space
  - internet data is typically big data    
&lt;br&gt;
- private, safe storage spaces
  - due to local rules, institutional requirements

---
name: part3
class: inverse, center, middle

# Part 3. Webpage Source Code

.footnote[

[Back to the contents slide](#contents-slide).

]

---
## Webpage Source Code &amp;mdash; Overview

- Webpages include more than what is immediately visible to visitors

  - not only text, images, links
  - but also code for structure, style, and functionality &amp;mdash; interpreted by browsers first
      - HTML provides the structure
      - CSS provides the style
      - JavaScript provides functionality, if any
      
&lt;br&gt;

- Web scraping requires working with the source code

   - even when harvesting the visible only
   - but source code is rarely a nuisance    
&lt;br&gt;
      - allows choosing one or more desired parts of the visible
           - e.g., text in table and/or bold only    
&lt;br&gt;
      - offers more, invisible, data
           - e.g., URLs hidden under text
           
---
## Webpage Source Code &amp;mdash; View in Browser

The `Ctrl` `+` `U` shortcut is to display source code &amp;mdash; alternatively, right click and `View` `Page` `Source`


&lt;img src="scrp_workshop_files/images_data/homepage.png" width="45%" /&gt;&lt;img src="scrp_workshop_files/images_data/homepage_source.png" width="45%" /&gt;


---
## Webpage Source Code &amp;mdash; View in Browser &amp;mdash; DOM

Browsers also offer putting source codes in a structure
- known as DOM (document object model), initiated by the `F12` key on Chrome

&lt;img src="scrp_workshop_files/images_data/homepage.png" width="45%" /&gt;&lt;img src="scrp_workshop_files/images_data/homepage_dom.png" width="45%" /&gt;

---
class: action

## Exercises

4) View the source code of a page
- as plain code and as in DOM
- compare the look of the two

&lt;br&gt;

5) Search for a word or a phrase in source code
- copy from the front-end page
- search in plain text code or in DOM
   - the `Ctrl` `+` `F`     
&lt;br&gt;   
- compare the look of the front- and back-end

<div class="countdown" id="timer_60a7865b" style="top:0;right:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">05</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---
## Webpage Source Code &amp;mdash; HTML &amp;mdash; Overview

.pull-left[

- HTML stands for Hypertext Markup Language

   - it gives the structure to what is visible to visitors
       - text, images, links      
&lt;br&gt;
   - would a piece of text appear in a paragraph or a list?
       - depends on the HTML code around that text
   
]

.pull-right[

```md
&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;style&gt;
      h1 {color: blue;}
    &lt;/style&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;h1&gt;A header&lt;/h1&gt;      
    &lt;p&gt;This is a paragraph.&lt;/p&gt;
    &lt;ul&gt;
       &lt;li&gt;This&lt;/li&gt;
       &lt;li&gt;is a&lt;/li&gt;
       &lt;li&gt;list&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/body&gt;
&lt;/html&gt;

```
]

---
## Webpage Source Code &amp;mdash; HTML &amp;mdash; Syntax

HTML consists of .yellow-h[elements]

```md
`&lt;p&gt;This is a one sentence paragraph.&lt;/p&gt;`

```

.out-t[

This is a one sentence paragraph.

]


&lt;br&gt;

Note that

- there is only one element on this page
   - a paragraph

---
## Webpage Source Code &amp;mdash; HTML &amp;mdash; Syntax

Most elements have opening and closing .yellow-h[tags]

```md
`&lt;p&gt;`This is a one sentence paragraph.`&lt;/p&gt;`

```

.out-t[

This is a one sentence paragraph.

]


&lt;br&gt;

Note that

- tag name, in this case .yellow-h[p], defines the structure of the element
- the closing tag has a forward slash .yellow-h[/] before the element name

---
## Webpage Source Code &amp;mdash; HTML &amp;mdash; Syntax

Most elements have some content

```md
&lt;p&gt;`This is a one sentence paragraph.`&lt;/p&gt;

```

.out-t[

This is a one sentence paragraph.

]

---
## Webpage Source Code &amp;mdash; HTML &amp;mdash; Syntax

Elements can be nested

```md
&lt;p&gt;This is a &lt;strong&gt;one&lt;/strong&gt; sentence paragraph.&lt;/p&gt;

```

.out-t[

&lt;p&gt;This is a &lt;strong&gt;one&lt;/strong&gt; sentence paragraph.&lt;/p&gt;
]


&lt;br&gt;

Note that

- there are two elements above, a paragraph and a strong emphasis
- strong is said to be the child of the paragraph element
   - there could be more than one child
   - in that case, children are numbered from the left      
&lt;br&gt;   
- paragraph is said to be the parent of the strong element

---
## Webpage Source Code &amp;mdash; HTML &amp;mdash; Syntax

Elements can have .yellow-h[attributes]

```md
&lt;p&gt;This is a &lt;strong `id="sentence-count"`&gt;one&lt;/strong&gt; sentence paragraph.&lt;/p&gt;

```

.out-t[
&lt;p&gt;This is a &lt;strong id="sentence-count"&gt;one&lt;/strong&gt; sentence paragraph.&lt;/p&gt;
]


&lt;br&gt;

Note that

- the id attribute is not visible to the visitors
- attribute string .yellow-h[sentence-count] could have been anything I could come up with
    - unlike the tag and attribute names &amp;mdash; e.g., strong, id as they are pre-defined      
&lt;br&gt;    
- there are some other attributes that are visible


---
## Webpage Source Code &amp;mdash; HTML &amp;mdash; Syntax

There could be more than one attribute in an element

```md
&lt;p&gt;This is a &lt;strong class="count" id="sentence-count"&gt;one&lt;/strong&gt; sentence paragraph.&lt;/p&gt;

&lt;p&gt;There are now &lt;strong class="count" id="paragraph-count"&gt;two&lt;/strong&gt; paragraphs.&lt;/p&gt;

```

.out-t[
&lt;p&gt;This is a &lt;strong class="count" id="sentence-count"&gt;one&lt;/strong&gt; sentence paragraph.&lt;/p&gt;

&lt;p&gt;There are now &lt;strong class="count" id="paragraph-count"&gt;two&lt;/strong&gt; paragraphs.&lt;/p&gt;

]


&lt;br&gt;

Note that

- there could be more than one attribute in an element
    - with a white space in between them    
&lt;br&gt;
- the `class` attribute can apply to multiple elements
   - while the `id` attibute must be unique on a page

---
## Webpage Source Code &amp;mdash; HTML &amp;mdash; Important Elements &amp; Attibutes 

Links

```md
&lt;p&gt;Click &lt;a href="https://www.google.com/"&gt;here&lt;/a&gt; to google things.&lt;/p&gt;

```

.out-t[
&lt;p&gt;Click &lt;a href="https://www.google.com/"&gt;here&lt;/a&gt; to google things.&lt;/p&gt;
]

&lt;br&gt;

Note that

- `href` (hypertext reference) is a required attribute for the the `a` (anchor) tag
- most attributes are optional, some are required

---
## Webpage Source Code &amp;mdash; HTML &amp;mdash; Links 

Links

```md
&lt;p&gt;Click &lt;a title="This text appears when visitors hover over the link" 
            href="https://www.google.com/"&gt;here&lt;/a&gt; to google things.&lt;/p&gt;

```

.out-t[
&lt;p&gt;Click &lt;a title="This text appears when visitors hover over the link" 
            href="https://www.google.com/"&gt;here&lt;/a&gt; to google things.&lt;/p&gt;
]

&lt;br&gt;

Note that

- the `a` (anchor) tag is used with `href` (hypertext reference)


---
## Webpage Source Code &amp;mdash; HTML &amp;mdash; Lists

The `&lt;lu&gt;` tag introduces unordered lists, while the `&lt;li&gt;` tag defines lists items


```r
&lt;ul&gt;
  &lt;li&gt;books&lt;/li&gt;
  &lt;li&gt;journal articles&lt;/li&gt;
  &lt;li&gt;reports&lt;/li&gt;
&lt;/ul&gt;
```

.out-t[

&lt;ul&gt;
  &lt;li&gt;books&lt;/li&gt;
  &lt;li&gt;journal articles&lt;/li&gt;
  &lt;li&gt;reports&lt;/li&gt;
&lt;/ul&gt;


]

&lt;br&gt;

Note that

- Ordered lists are introduced with the the `&lt;lo&gt;` tag instead

---
## Webpage Source Code &amp;mdash; HTML &amp;mdash; Notes

By default, multiple spaces and/or lines breaks are not meaningful


```r
&lt;ul&gt;&lt;li&gt;books&lt;/li&gt;&lt;li&gt;journal                     articles&lt;/li&gt;&lt;li&gt;reports
&lt;/li&gt;
  
  
&lt;/ul&gt;
```

.out-t[

&lt;ul&gt;&lt;li&gt;books&lt;/li&gt;&lt;li&gt;journal                     articles&lt;/li&gt;&lt;li&gt;reports
&lt;/li&gt;
  
  
&lt;/ul&gt;

]

&lt;br&gt;

Note that

- plain source code may or may not be written in a readable manner
- this is one reason why DOM is helpful

---
## Webpage Source Code &amp;mdash; CSS &amp;mdash; Overview


- CSS stands for Cascading Stylesheets

   - it gives the style to what is visible to visitors
       - text, images, links      
&lt;br&gt;
   - would a piece of text appear in black or blue?
       - depends on the CSS for that text
       
&lt;br&gt;

- CSS can be defined
   
   - inline, as an attribute of an element
   - internally, as a child element of the `head` element
   - externally, but then linked in the `head` element

---
## Webpage Source Code &amp;mdash; CSS &amp;mdash; Syntax

.pull-left[
CSS is written in .yellow-h[rules]

]

.pull-right[
```md
`p {font-size:12px;}`

`.count {background-color:yellow;}`

`#sentence-count {color:red;}`

```

]

---
## Webpage Source Code &amp;mdash; CSS &amp;mdash; Syntax

.pull-left[
CSS is written in rules, with a syntax consisting of
- one or more .yellow-h[selectors]

]

.pull-right[
```md
`p` {font-size:14px;}

`h1 h2` {color:blue;}

`.count` {background-color:yellow;}

`#sentence-count` {color:red; font-size:14px;}

```
]

&lt;br&gt;

Note that

- selector type defines the syntax
   - elements are plain
       - e.g., p, h1, h2
   - classes start with a full stop
   - ids start with a number sign

---
## Webpage Source Code &amp;mdash; CSS &amp;mdash; Syntax

.pull-left[
CSS is written in rules, with a syntax consisting of
- one or more selectors
- a .yellow-h[decleration]

]

.pull-right[
```md
p `{font-size:14px;}`

h1 h2 `{color:blue;}`

.count `{background-color:yellow;}`

#sentence-count `{color:red; font-size:14px;}`

```
]

&lt;br&gt;

Note that

- declerations are written in between two curly brackets

---
## Webpage Source Code &amp;mdash; CSS &amp;mdash; Syntax

.pull-left[
CSS is written in rules, with a syntax consisting of
- one or more selectors
- a decleration, with one or more .yellow-h[properties]

]

.pull-right[
```md
p {`font-size:`14px;}

h1 h2 {`color:`blue;}

.count {`background-color:`yellow;}

#sentence-count {`color:`red;` font-size:`14px;}

```
]

&lt;br&gt;

Note that

- properties are followed by a colon

---
## Webpage Source Code &amp;mdash; CSS &amp;mdash; Syntax

.pull-left[
CSS is written in rules, with a syntax consisting of
- one or more selectors
- a decleration, with one or more properties and .yellow-h[values]

]

.pull-right[
```md
p {font-size:`14px;`}

h1 h2 {color:`blue;`}

.count {background-color:`yellow;`}

#sentence-count {color:`red;` font-size:`14px;`}

```
]

&lt;br&gt;

Note that

- values are followed by a semicolon
- `property:value;` pairs are seperated by a white space


---
## Webpage Source Code &amp;mdash; CSS &amp;mdash; Internal

.pull-left[

- CSS rules can be defined internally
   - within the `style` element
   - as a child of the `head` element

- Internally defined rules apply to all matching selectors
   - on the same page
]

.pull-right[


```r
&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
*   &lt;style&gt;
*     h1 {color: blue;}
*   &lt;/style&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;h1&gt;A header&lt;/h1&gt;      
    &lt;p&gt;This is a paragraph.&lt;/p&gt;
    &lt;ul&gt;
       &lt;li&gt;This&lt;/li&gt;
       &lt;li&gt;is a&lt;/li&gt;
       &lt;li&gt;list&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/body&gt;
&lt;/html&gt;
```
]

---
## Webpage Source Code &amp;mdash; CSS &amp;mdash; External

.pull-left[

- CSS rules can be defined externally
   - saved somewhere linkable
   - defined with the the `linked` element
   - as a child of the `head` element

- Externally defined rules 
   - are saved in a file with .css extension
   - apply to all matching selectors
       - on any page linked
]


.pull-right[


```r
&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
*   &lt;link rel="styles" href="simple.css"&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;h1&gt;A header&lt;/h1&gt;      
    &lt;p&gt;This is a paragraph.&lt;/p&gt;
    &lt;ul&gt;
       &lt;li&gt;This&lt;/li&gt;
       &lt;li&gt;is a&lt;/li&gt;
       &lt;li&gt;list&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/body&gt;
&lt;/html&gt;
```
]

---
## Webpage Source Code &amp;mdash; CSS &amp;mdash; Inline

CSS rules can also be defined inline
- with the `style` attribute
- does not require selector
- applies only to that element

```md   
&lt;p&gt;This is a &lt;strong `style="color:blue;"`&gt;one&lt;/strong&gt; sentence paragraph.&lt;/p&gt;
``` 
   
.out-t[
&lt;p&gt;This is a &lt;strong style="color:blue;"&gt;one&lt;/strong&gt; sentence paragraph.&lt;/p&gt;
]

---
name: part4
class: inverse, center, middle

# Part 3. Scraping Static Pages

.footnote[

[Back to the contents slide](#contents-slide).

]

---
## Scraping Static Pages &amp;mdash; Overview

- We will collect data from static pages with the `rvest` package

   - static pages are those that display the same source code
       - including the content &amp;mdash; it does not change    
&lt;br&gt;       
   - every visitor sees the same page at a given URL
   - each page has a different URL

&lt;br&gt;

- Scraping static pages involves two main tasks

   - download the source code from one or more page to R
      - typically, the only interaction with the page itself     
&lt;br&gt;
   - select the exact information needed from the source code
      - takes place locally, on your machine
      - the main functionality that `rvest` offers
      - working with selectors
    
---
## Scraping Static Pages &amp;mdash; `rvest` &amp;mdash; Overview

- A relative small R package for web scraping

  - created by [Hadley Wickham](http://http://hadley.nz/)
  - popular &amp;mdash; used by many for web scraping
      - downloaded 639,546 times last month     
&lt;br&gt;      
  - last major revision was in March 2021
      - better alignment with `tidyverse`

--

&lt;br&gt;

- A lot has already been written on this package

  - you will find solutions to, or help for, any issues online
  - see first the [package documentation](https://cran.r-project.org/web/packages/rvest/rvest.pdf), numerous tutorials &amp;mdash; such as [this](https://rvest.tidyverse.org/) and [this](https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/), and [this](https://steviep42.github.io/webscraping/book/index.html#quick-rvest-tutorial)
     

--

&lt;br&gt;

- Comes with the recommendation to combine it with the `polite` package

   - for ethical web scraping

---
## Scraping Static Pages &amp;mdash; `rvest` &amp;mdash; Get Source Code

Use the `read_html` function to get the source code of a page into R


```r
read_html("https://parliament-luzland.netlify.app/")
```

```
## {html_document}
## &lt;html lang="en-us"&gt;
## [1] &lt;head&gt;\n&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;\n&lt;meta charset="utf-8"&gt;\n&lt;m ...
## [2] &lt;body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" class="page-wrapper"&gt;\n&lt;sc ...
```

---
## Scraping Static Pages &amp;mdash; `rvest` &amp;mdash; Get Source Code

You may wish to check the protocol first


```r
paths_allowed("https://parliament-luzland.netlify.app/")
```

```
## [1] TRUE
```

```r
read_html("https://parliament-luzland.netlify.app/")
```

```
## {html_document}
## &lt;html lang="en-us"&gt;
## [1] &lt;head&gt;\n&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;\n&lt;meta charset="utf-8"&gt;\n&lt;m ...
## [2] &lt;body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" class="page-wrapper"&gt;\n&lt;sc ...
```














---
name: part5
class: inverse, center, middle

# Part 4. Scraping Dynamic Pages

.footnote[

[Back to the contents slide](#contents-slide).

]

---
## Data Preperation &amp;mdash; Overview

- The `rtweet` package does a very good job with data preperation to start with

   - returns data frames, with mostly tidy data
   - although Twitter APIs return nested lists
   - some variables are still lists
      - e.g., `hastags`

--

&lt;br&gt;

- Further data preparation depends on your research project

   - most importantly, on whether you will work with texts or not
   - we will cover some common preparation steps

---
## Data Preperation &amp;mdash; Overview &amp;mdash; Strings

- Most researchers would be interested in textual Twitter data

   - tweets as a whole, but also specifically hashtags *etc*.

--

&lt;br&gt;

- There are many components of tweets as texts

   - e.g., mentions, hashtags, emojis, links *etc*.
   - but also punctuation, white spaces, upper case letters *etc*.
   - some of these may need to be taken out before analysis

--

&lt;br&gt;

- I use the `stringr` package ([Wickham, 2019](https://cran.r-project.org/web/packages/stringr/index.html)) for string operations

   - part of the `tidyverse` family
   - you might have another favourite already
       - no need to change as long as it does the job
       
---
## Data Preperation &amp;mdash; Overview &amp;mdash; Numbers

- There is more to Twitter data than just tweets
      
  - e.g., the number of followers, likes *etc*.
     - see Silva and Proksch ([2020](https://doi.org/10.1017/S0003055420000817)) for a great example

--
&lt;br&gt;

- I use the `dplyr` package ([Wickham et al, 2020](https://cran.r-project.org/web/packages/dplyr/index.html) for most data operations

   - part of the `tidyverse` family
   - you might have another favourite already
       - no need to change as long as it does the job

---
## Data Preperation &amp;mdash; Remove Mentions


```r
tweet &lt;- "These from @handle1 are #socool. üëè A #mustsee, @handle2! 
          üëâ t.co/aq7MJJ1
          üëâ https://t.co/aq7MJJ2"
```


```r
str_remove_all(string = tweet, pattern = "[@][\\w_-]+")
```


.out-t[

[1] "This from  are #socool. üëè A #mustsee, ! 
          üëâ t.co/aq7MJJ1
          üëâ https://t.co/aq7MJJ2"

]


---
## Data Preperation &amp;mdash; Remove Hashtags



```r
tweet &lt;- "These from @handle1 are #socool. üëè A #mustsee, @handle2! 
          üëâ t.co/aq7MJJ1
          üëâ https://t.co/aq7MJJ2"
```


```r
str_remove_all(string = tweet, pattern = "[`#`][\\w_-]+")
```


.out-t[

[1] "These from @handle1 are . üëè A , @handle2! 
          üëâ t.co/aq7MJJ1
          üëâ https://t.co/aq7MJJ2"

]

---
## Data Preperation &amp;mdash; Remove Links


```r
tweet &lt;- "These from @handle1 are #socool. üëè A #mustsee, @handle2! 
          üëâ t.co/aq7MJJ1
          üëâ https://t.co/aq7MJJ2"
```


```r
str_remove_all(string = tweet, pattern = "http\\S+\\s*")
```


.out-t[

[1] "These from @handle1 are. üëè A, @handle2! 
          üëâ t.co/aq7MJJ1"

]

&lt;br&gt;

- Notice that

  - links come in various formats
  - you may need multiple or complicated regular expression patterns

---
## Data Preperation &amp;mdash; Remove Links &amp;mdash; Alternative

Use the `urls_t.co` variable to remove all links

- if there are more than one link in a tweet, they are stored as a list in this variable



```r
# start with your existing dataset of tweets
df_tweets &lt;- df_tweets %&gt;%
  
# limit the operation to within individual tweets  
  group_by(status_id) %&gt;%
  
# create a new variable of tweets without links  
  mutate(tidy_text = 
           
# by removing them from the existing variable `text`           
           str_remove_all(string = text, 
 
# that matches the `urls_t.co` variable, after being collapsed into a string      
           pattern = str_c(unlist(urls_t.co), collapse = "|")))
```

--
class: action

<div class="countdown" id="timer_60a78405" style="top:0;right:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">08</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---
## Data Preperation &amp;mdash; Remove Emojis


```r
tweet &lt;- "These from @handle1 are #socool. üëè A #mustsee, @handle2! 
          üëâ t.co/aq7MJJ1
          üëâ https://t.co/aq7MJJ2"
```


```r
iconv(x = tweet, from = "latin1", to = "ASCII", sub = "")
```


.out-t[

[1] "These from @handle1 are #socool. A #mustsee, @handle2! 
          t.co/aq7MJJ1
          https://t.co/aq7MJJ2"

]


---
## Data Preperation &amp;mdash; Exercises &amp;mdash; Notes


- The exercises in this part are best followed by

  - using `tweets.rds` or similar dataset
  - saving a new variable at every step of preparation
  - observing the newly created variables 
      - to confirm whether the code works as intended

&lt;br&gt;

- The `mutate` function, from the `dplyr` package, can be helpful, as follows

   - recall that `text` is the variable for tweets


```r
tweets &lt;- read_rds("data/tweets.rds")

clean_tweets &lt;- tweets %&gt;%
  `mutate(no_mentions` = str_remove_all(string = `text`, pattern = "[@][\\w_-]+"))
```

---
class: action

## Exercises


41) Remove mentions

- hint: the pattern is `"[@][\\w_-]+"`

42) Remove hastags
- hint: the pattern is `"[#][\\w_-]+"`

43) Remove links
- by using the links from the `urls_t.co` variable

44) Remove emojis
- pull the help file for the `iconv` function first


<div class="countdown" id="timer_60a78778" style="top:0;right:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">10</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>


---
## Data Preperation &amp;mdash; Remove Punctuations


```r
tweet &lt;- "These from @handle1 are #socool. üëè A #mustsee, @handle2! 
          üëâ t.co/aq7MJJ1
          üëâ https://t.co/aq7MJJ2"
```


```r
str_remove_all(string = tweet, pattern = "[[:punct:]]")
```


.out-t[

[1] "This from are socool üëè A mustsee handle2
          üëâ tcoaq7MJJ1
          üëâ httpst.coaq7MJJ2"

]

Notice that
   - this removed all punctuation, including those in mentions, hashtags, and links
   - if tweets are typed with no spaces after punctuation, this might lead to merged pieces of text
      - alternatively, try the `str_replace` function to replace punctuation with space
      
---
## Data Preperation &amp;mdash; Remove Punctuations &amp;mdash; Alternative


```r
tweet &lt;- "This is a sentence.There is no space before this sentence."
```


```r
str_`remove`_all(string = tweet, pattern = "[[:punct:]]")
```

.out-t[

[1] "This is a sentenceThere is no space before this sentence"

]

--

&lt;br&gt;



```r
str_`replace`_all(string = tweet, pattern = "[[:punct:]]", replacement = " ")
```

.out-t[

[1] "This is a sentence There is no space before this sentence "

]

---
## Data Preperation &amp;mdash; Remove Punctuations &amp;mdash; Alternative



```r
tweet &lt;- "This is a sentence.There is no space before this sentence."
```


```r
str_replace_all(string = tweet, pattern = "[[:punct:]]", replacement = " ")
```

.out-t[

[1] "This is a sentence There is no space before this sentence "

]

---
## Data Preperation &amp;mdash; Remove Repeated Whitespace



```r
tweet &lt;- "There are too many spaces after this sentence.   This is a new sentence."
```


```r
str_squish(string = tweet)
```

.out-t[

[1] "There are too many spaces after this sentence. This is a new sentence."

]

Note that
- white spaces can be introduced not only by users on Twitter, but also by us, while cleaning the data
   - e.g., removing and/or replacing operations above
   - hence, this function might be useful after other operations

---
## Data Preperation &amp;mdash; Change Case


```r
tweet &lt;- "lower case. Sentence case. Title Case. UPPER CASE."
```


```r
str_to_lower(string = tweet)
```

.out-t[

[1] "lower case. sentence case. title case. upper case."

]

Note that
   - there are other functions in this family, including
      - `str_to_sentence`, `str_to_title`, `str_to_upper`


---
class: action

## Exercises

45) Remove punctuations
- by using the `str_replace_all` function
- hint: the pattern is `[[:punct:]]`

&lt;br&gt;

46) Remove whitespace
- hint: the function is called `str_squish`

&lt;br&gt;

47) Change case to lower case
- hint: the function is called `str_to_lower`


<div class="countdown" id="timer_60a785de" style="top:0;right:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">10</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---
## Data Preperation &amp;mdash; Change Unit of Observation

Research designs might require changing the unit of observation
   - aggregation
      - e.g., at the level of users, locations, hashtags etc.
      - summarise with `dplyr`
   

  - dis-aggregation
     - e.g., to the level of words
     - tokenise with `tidytext`
     
---
## Data Preperation &amp;mdash; Change Unit of Observation &amp;mdash; Aggregation

Aggregate at the level of users

- the number of tweets per user


```r
# load the tweets dataset
df &lt;- read_rds("tweets.rds") %&gt;%
  
# group by users for aggregation  
  group_by(user_id) %&gt;%
  
# create summary statistics for variables of interest
  summarise(sum_tweets = n())
```

---
## Data Preperation &amp;mdash; Change Unit of Observation &amp;mdash; Aggregation

What is aggregated at which level depends on your research design, such as
   - aggregate the tweets into a single text
   - at the level of users by source



```r
# load the tweets dataset
df &lt;- read_rds("tweets.rds") %&gt;%
  
# group by users for aggregation  
  group_by(user_id, `source`) %&gt;%
  
# create summary statistics for variables of interest
  summarise(`merged_tweets = paste0(text, collapse = ". ")`) 
```

---
## Data Preperation &amp;mdash; Change Unit of Observation &amp;mdash; Dis-aggregation

Disaggregate the tweets, by splitting them into smaller units
  - also called .yellow-h[tokenisation]

Note that
- by default `sep = "[^[:alnum:].]+"`, which works well with separating tweets into words
   - change this argument with a regular expression of your choice
- this creates a tidy dataset, where each observation is a word
   - all other tweet-level variables are repeated for each observation


```r
# load the tweets dataset
df &lt;- read_rds("tweets.rds") %&gt;%
  
# split the variable `text`  
    separate_rows(text)
```

---
## Data Preperation &amp;mdash; Change Unit of Observation &amp;mdash; Dis-aggregation

The `tidytext` has a function that works better with tokenising tweets
- with `token = "tweets"`, it dis-aggregates text into words
   - except that it respects usernames, hashtags, and URLS 


```r
# load the tweets dataset
df &lt;- read_rds("tweets.rds") %&gt;%
  
# split the variable `text`, create a new variable called `da_tweets`    
    unnest_tokens(output = da_tweets, input = text, token = "tweets")
```

---
## Data Preperation &amp;mdash; Change Unit of Observation &amp;mdash; Dis-aggregation

Tokenise variables to levels other than words
- e.g., characters, words (the default),  sentences, lines


```r
# load the tweets dataset
df &lt;- read_rds("tweets.rds") %&gt;%
  
# split the variable `text` into sentences, create a new variable called `da_tweets`    
    unnest_tokens(output = da_tweets, input = text, `token = "sentences"`)
```

---
## Data Preperation &amp;mdash; Change Unit of Observation &amp;mdash; Dis-aggregation

Tokenise variables other than tweets
- recall that `rtweet` stores multiple hastags, mentions *etc*. as lists



```r
# load the tweets dataset
df &lt;- read_rds("tweets.rds") %&gt;%
  
# unlist the lists of hashtags to create strings  
  group_by(status_id) %&gt;%
  mutate(tidy_hashtags = str_c(unlist(hashtags), collapse = " ")) %&gt;%
  
# split the string, create a new variable called `da_tweets`    
  unnest_tokens(output = da_hashtags, input = tidy_hashtags, token = "words")
```

---
## Data Preperation &amp;mdash; Remove Stop Words

Remove the common, uninformative words
- e.g., the, a, i

Note that 
- this operation requires a tokenised-to-word variable
- stop words for English are stored in the `stop_words` dataset in the `tidytext` variable
- list of words for other languages are available elsewhere, including 
   - the `stopwordslangs` function from the `rtweet` package
   - the `stopwords` function from the `tm` package
      - e.g., use `tm::stopwords("german")` for German


```r
# load the tweets dataset
df &lt;- read_rds("tweets.rds") %&gt;%
  
# split the variable `text`, create a new variable called `da_tweets`    
  unnest_tokens(output = da_tweets, input = text, token = "tweets") %&gt;%
 
# remove rows that match any of the stop words as stored in the stop_words dataset 
  anti_join(stop_words, by = c("da_tweets" = "word"))
```

---
class: action

## Exercises

48) Aggregate `text` to a higher level
- e.g., if you are not using `tweets.rds`, to MP level
   - if not, perhaps to `source` level

&lt;br&gt;

49) Dis-aggregate `text` to a lower level
- e.g., to words

&lt;br&gt;

50) Dis-aggregate `hashtags`
- i.e., make sure each row has at most one hashtag

&lt;br&gt;

51) Remove stop words


<div class="countdown" id="timer_60a7856f" style="top:0;right:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">10</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---
name: reference-slide
class: inverse, center, middle

# References

.footnote[

[Back to the contents slide](#contents-slide).

]

---
## References

Harrison, J. (2020). [RSelenium: R Bindings for 'Selenium WebDriver'](https://cran.r-project.org/web/packages/RSelenium/index.html). R package, version 1.7.7.

Meissner, P., &amp; Ren, K. (2020). [robotstxt: A 'robots.txt' Parser and 'Webbot'/'Spider'/'Crawler' Permissions Checker](https://cran.r-project.org/web/packages/robotstxt/index.html). R package, version 0.7.13.

Perepolkin, D. (2019). [polite: Be Nice on the Web](https://cran.r-project.org/web/packages/polite/index.html). R package, version 0.1.1.

Robinson, D., &amp; Silge, J. (2021). [tidytext: Text mining using 'dplyr', 'ggplot2', and other tidy tools](https://cran.r-project.org/web/packages/tidytext/index.html). R package, version 0.3.0.

Silge, J., &amp; Robinson, D. (2017). [Text mining with R: A tidy approach](https://www.tidytextmining.com). O'Reilly. Open access at

Wickham, H. (2019). [stringr: Simple, Consistent Wrappers for Common String Operations](https://cran.r-project.org/web/packages/stringr/index.html). R package, version 1.4.0.

Wickham, H. (2021). [rvest: Easily Harvest (Scrape) Web Pages](https://cran.r-project.org/web/packages/rvest/index.html). R package, version 1.0.0.




Wickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., Woo, K., Yutani, H. and Dunnington, D. (2020). [dplyr: A grammar of data manipulation](https://cran.r-project.org/web/packages/dplyr/index.html). R package, version 0.8.5.

Wickham, H. and Grolemund, G. (2019). [R for data science](https://r4ds.had.co.nz). O'Reilly. Open access at [https://r4ds.had.co.nz](https://r4ds.had.co.nz/).

Wickham, H., RStudio (2019). [https://cran.r-project.org/web/packages/tidyverse/index.html](tidyverse: Easily Install and Load the 'Tidyverse'). R package, version 3.3.3.

Xie, Y. (2020). [xaringan: Presentation Ninja](https://cran.r-project.org/web/packages/xaringan/index.html). R package, version 0.19.

---
class: middle, center

## The workshop ends here.
## Congradulations for making it this far, and
## thank you for joining me!

.footnote[

[Back to the contents slide](#contents-slide).

]


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"highlightSpans": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
